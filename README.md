<h1 align="center">Hello, I'm <a href="https://github.com/sazhirom">George Romanov</a> ğŸ‘‹</h1>

<p align="center">
  <img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExaGJuc2J1YjExMm9jdDF4bGhkaGF3ZGg0bXkyYzRvdDQ3c25qYXk3biZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/4k9BkIfSbgr2LTRB8P/giphy.gif" width="180"/>
  <img src="https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExdnFxY2hibGhhZHRoZGpoeTZocnhneWxjM2h0ZXFjNXVxYmQzd3k3OSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/ySeD2PB1OfMSKFEheH/giphy.gif" width="180"/>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Business%20Analyst-323330?style=for-the-badge&logo=soundcharts&logoColor=white" alt="Business Analyst Badge">
  <img src="https://img.shields.io/badge/ETL%20Developer-4B8BBE?style=for-the-badge&logo=docker&logoColor=white" alt="ETL Developer Badge"> 
</p>

---

## ğŸ“‹ Table of Contents

- [ğŸ“Š Dashboards (*Tableau + Grafana)*](#dashboards)
- [ğŸ”¥ Complex ETL Projects](#etl)
  - [ğŸ’° Betting Analyzer (*Python: BS4, Selenium, Pandas, NumPy â†’ GoogleCloud â†’ Airflow, Redis â†’ Kafka â†’ ClickHouse â†’ Grafana)*](#betting)
- [ğŸ“ˆ Smaller Projects](#small)
  - [ğŸ† Kaggle Competition - Child Mind (*Python: Pandas, SNS, matplotlib, LGBM*)](#kaggle)
  - [ğŸ“š World Global Values survey analysis (*Python: Pandas - Tableau*)](#WVS)
  - [ğŸ” Airflow - Dynamic Dag (*Airflow, Redis*)](#Airflow)
  - [ğŸ§  Sports Odds Modeling (*CatBoost, ClickHouse*)](#Catboost)
- [ğŸ› ï¸ Skills](#skills-section)
- [ğŸ’¼ Work Experience](#experience-section)
- [ğŸ“¬ Contacts](#contacts-section)

---
<br>
<br>

<a id="Dashboards"></a>
## ğŸ“Š Dashboards

Main Showcase for Dashboards is here - [**Tableau Profile**](https://public.tableau.com/app/profile/george.romanov/vizzes)
<br>
<br>  


| **Dashboard**         | **Description**                                  | **Link** |
|----------------------|--------------------------------------------------|------------------|
| ![**Hotel Survey / Linker Diagram**](https://github.com/sazhirom/Tableau/blob/main/hotel.png?raw=true)  | Dashboard featuring a Linker Diagram, custom filters, and highlight actions. Based on synthetic data generated with Python Faker.  | [**Link to Tableau Public**](https://public.tableau.com/app/profile/george.romanov/viz/HotelSurveyLinkerExample/Dashboard1) |
| ![**Marketing / Funnel Diagram**](https://github.com/sazhirom/Tableau/blob/main/marketing.png?raw=true)  | Marketing Funnel, custom shapes and colours, scatter plots, custom list-based filters, dynamic sets, a lot of conditional formatting and so on.   | [**Link to Tableau Public**](https://public.tableau.com/app/profile/george.romanov/viz/MarketingDashboard_17453783792250/Dashboard1) |
| ![**Purchasing Power: Europe Comparison**](https://github.com/sazhirom/Tableau/blob/main/costliving.png?raw=true)  | Not overly complex, but illustrates a histogram and a map linked with the histogram, highlight actions, and the ability to customize maps. Data from Numbeo.  | [**Link to Tableau Public**](https://public.tableau.com/app/profile/george.romanov/viz/CostoflivingEurope/Dashboard1) |
| ![**KPI Dashboard**](https://github.com/sazhirom/Tableau/blob/main/mini.png?raw=true)  | Allows dynamic month-to-month KPI comparisons and year-over-year running sum analysis across all key metrics. The most complex feature involves advanced formulas with fixed views to display a running sum of percentage values, illustrating the cumulative profit ratio. | [**Link to Tableau Public**](https://public.tableau.com/app/profile/george.romanov/viz/MinimalisticDashboardwithKPI/Dashboard1) |
| ![**Green Energy**](https://github.com/sazhirom/Tableau/blob/main/green.png?raw=true)  | Compares the share of green energy in the top 10 GDP countries, with the ability to view both absolute and relative values. | [**Link to Tableau Public**](https://public.tableau.com/app/profile/george.romanov/viz/GlobalGreenEnergyTrends/Dashboard1) |
| ![**Betting Analyzer ETL**](https://github.com/sazhirom/Tableau/blob/main/grafana.PNG?raw=true)  | Grafana dashboard for that [ETL_project](#betting). Featuring conditinal colour formatiing for different values, bunch of optimization to reduce server load and a lot of SQL. | [View the Grafana Dashboard](http://138.2.100.167:3000/public-dashboards/5c64055a2d0a432aafe2d29dae512883) |


---
<br>
<br>

<a id="ETL"></a>

## ğŸ”¥ Complex ETL Projects
---
<a id="betting"></a>

### ğŸ’° Betting Analyzer  
ğŸ“ˆ [View the Grafana Dashboard](http://138.2.100.167:3000/public-dashboards/5c64055a2d0a432aafe2d29dae512883)



---

#### ğŸ¯ Project Goal
A system that **synchronizes live odds from the three largest betting sites** and identifies the best betting opportunities.

#### ğŸ” Challenges & Solutions
- **Website restrictions**: Platforms are protected from scraping and require authorization.  
- **Limited server resources**: The 1-2 GB RAM servers required **code optimization** and **efficient memory management**.  
- **Real-time data analysis**: Data must be **collected synchronously with random timing**â€”requiring a **coordinator and error handling system**.  
- **Optimizing ClickHouse for 4GB RAM**: ClickHouse is not designed for small instances, requiring fine-tuning for stability.  

---

#### ğŸ› ï¸ Project Data Architecture
![Project Diagram](https://raw.githubusercontent.com/sazhirom/images/refs/heads/main/mermaid-diagram-2025-01-08-082426.svg)

---
#### ğŸ”§ Implementation  
The system consists of **five servers**:
1. **Coordinator server**:
   - A **Python script** monitors scraper readiness, generates random parsing intervals, manages event triggers, and restarts instances when failures occur.  
   - **Redis** is used for fast data exchange.  
   - **Airflow** manages task scheduling and recovery.  
   - **Kafka Producer** sends data to the database server.  
   - A **Python processing script** matches events and selects the best betting odds.  
   - **Grafana dashboard** visualizes results from ClickHouse.  
2. **Three scraper servers**:
   - Collect live betting odds at specified intervals.  
   - Also scrape **pre-match** odds and match results.  
3. **Database server**:
   - **Kafka Receiver** for incoming data.  
   - **ClickHouse** to store and process betting data.  

#### ğŸ› ï¸ Technologies  
- **Programming**: Python  
- **Database**: ClickHouse, PostgreSQL (for Airflow)  
- **Infrastructure**: Google Cloud, Airflow automation, Redis, Kafka  
- **Visualization**: Grafana  

---

Updated **live** from **06:00 to 16:00 CET**:  
[Grafana Dashboard](http://138.2.100.167:3000/public-dashboards/5c64055a2d0a432aafe2d29dae512883)  

---

### ğŸ”¥ **Project Functions & Technologies**

| **Function**         | **Description**                                  | **Technologies** |
|----------------------|--------------------------------------------------|------------------|
| [**Data Scraping**](https://github.com/sazhirom/ibet#ibet-scrape)  | Scraping data from three betting websites and storing in Redis. Also scrapes match results.  | Python, Selenium, BeautifulSoup, Pandas, regex |
| [**Data Processing**](https://github.com/sazhirom/ibet#ibet-merge)  | Aggregates data from Redis. Finds matching events via transliteration & custom functions. | Pandas, NumPy, regex |
| [**VNC Server**](https://github.com/sazhirom/ibet#ibet-VNC) | VNC setup for login/captcha bypass. | VNC |
| [**Redis**](https://github.com/sazhirom/ibet#ibet-redis)  | Redis configuration for data transfer.  | Redis |
| [**Kafka**](https://github.com/sazhirom/ibet#ibet-kafka) | Kafka setup using KRaft mode for data transfer to ClickHouse. | Kafka |
| [**ClickHouse**](https://github.com/sazhirom/ibet#ibet-clickhouse) | Running and configuring ClickHouse on a 4GB RAM server. | ClickHouse, SQL |
| [**PostgreSQL**](https://github.com/sazhirom/ibet#ibet-postgres) | Used for Airflow stability & metadata storage. | PostgreSQL |
| [**Airflow**](https://github.com/sazhirom/ibet#ibet-airflow) | Workflow automation (8 DAGs). | Airflow |
| [**GCP & Bash**](https://github.com/sazhirom/ibet#ibet-bash) | Cloud setup, IAM permissions, logging, and environment configuration. | Bash, GCP CLI |
| [**Grafana**](https://github.com/sazhirom/ibet#ibet-grafana) | Creating a live analytics dashboard. | Grafana, SQL |

---
<br>
<br><br>


<a id="small"></a>
## ğŸ“ˆ Smaller projects

---

<a id="kaggle"></a>
### ğŸ† Kaggle competition - Child Mind - Pandas, SNS, matplotlib, LGBM 

**Objective**: Predict which children and teenagers may have problematic internet usage.  

**Data**: Anonymous survey data containing demographic, psychological, and behavioral information, helping to understand the relationship between various factors and susceptibility to problematic internet behavior.  
 
**Result**  

 Kaggle notebook link: [https://www.kaggle.com/code/georgiiromanov/child-mind-final](https://www.kaggle.com/code/georgiiromanov/child-mind-final) 
 
--- 
<br>

<a id="WVS"></a>
### ğŸ“š World Global Values survey analysis - Python: Pandas, Tableau

**Objective**: Analyze a large-scale survey to uncover meaningful insights.

**Data**: 270,000 survey responses from the World Values Survey (Wave 7, 2017â€“2022).

**Idea**: I aimed to identify the most significant generational gaps within a very specific demographic: middle-aged Europeans. I compared two closely matched groups â€” individuals aged 25â€“34 and those aged 55â€“64. Both groups share many similarities: most have children, stable jobs, and rely on the Internet as their primary source of news. This makes the differences between them especially interesting.

<details>
<summary>Python code</summary>
  
```python
import pandas as pd
import re

df = pd.read_csv(r'C:\Users\user\Desktop\tableau\world global quest\wvs.csv')


#only European countries
df = df[df["B_COUNTRY"].isin([
    8, 20, 40, 112, 56, 70, 100, 191, 203, 208, 233, 246, 250, 276,
    300, 348, 352, 372, 380, 428, 438, 440, 442, 470, 498, 492, 499,
    807, 528, 578, 616, 620, 642, 674, 688, 703, 705, 724, 752, 756,
    804, 826, 336
])]

# define cols with age and questions for analysis
cols_age = [col for col in df.columns if '003' in col][0]
cols_q = [col for col in df.columns if re.match(r'Q\d+', col)]

print(cols_q)

df1 = df.melt(id_vars = cols_age, value_vars = cols_q, value_name = "Answer",var_name = 'Question')
df1=df1[df1['Answer']>-1]

df1 = df1.groupby(list(df1.columns)).size().reset_index(name='count')
df1.rename(columns = {'X003R':'Age Group'},inplace = True)

df1 = df1[df1['Age Group']>0]
df1.head()

df1['total'] = df1.groupby(['Age Group', 'Question'])['count'].transform('sum')
df1 = df1[df1['Age Group'].isin([2,5])]

df1['ratio'] = df1['count']/df1['total']


df2 = df1
df_final = df1.merge(df2, on = ['Question','Answer'], how = 'inner')
df_final.head()

index_to_drop = df_final[df_final['total_x'] == df_final['total_y']].index

df = df_final.drop(index = index_to_drop)

df.head()

df['delta'] = round(df['ratio_y'] - df['ratio_x'],5)

df_positive = df.sort_values(by = 'delta', ascending = False).head(100)
df_positive.head()

df_negative = df.sort_values(by = 'delta', ascending = True).head(100)
df_negative.head()

df_negative.to_csv(r'C:\Users\user\Desktop\tableau\world global quest\negative.csv')

df1 = df.melt(id_vars = cols_age, value_vars = cols_q, value_name = "Answer",var_name = 'Question')
df1=df1[df1['Answer']>-1]

df1 = df1.groupby(list(df1.columns)).size().reset_index(name='count')
df1.rename(columns = {'X003R':'Age Group'},inplace = True)
df1 = df1[df1['Age Group']>0]
df1.head()

df1['total'] = df1.groupby(['Age Group', 'Question'])['count'].transform('sum')
df1 = df1[df1['Age Group'].isin([2,5])]
df1['ratio'] = df1['count']/df1['total']

df1.head()
df1.to_csv(r'C:\Users\user\Desktop\tableau\world global values\all_answers.csv')
```
</details>

<details>
  <summary>Tableau Dashboard (not my best work, but still worth sharing)</summary>
  
![Dashboard](https://github.com/sazhirom/Tableau/blob/main/values.png?raw=true)

</details>

--- 
<br>

<a id="Airflow"></a>
### ğŸ” Airflow - Dynamic dags
This is part of the legacy code from the ETL betting project.
The idea was simple: if a scraper failed, it would set the corresponding Redis variable to 0.
For example, if scraping from olimpbet.ru failed, the olimpbet variable in Redis would be set to 0.

Then, an Airflow DAG would check these variables every 3 minutes, restart the necessary DAGs if there is 0, and reset the variable to 1 after restarting.

The code is below.
(I eventually removed this part from the main project â€” it turned out to be easier to just refresh every page every 3 minutes rather than checking whether it needs refreshing.)

<details>
<summary>Dag code</summary>
  
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.redis.hooks.redis import RedisHook
from airflow.decorators import dag, task
from datetime import datetime, timedelta
import time

default_args = {
    'owner': 'airflow',
    'retries': 0,
    'start_date': days_ago(1)
}

@dag(
    dag_id="monitor",
    description="Global monitoring DAG",
    schedule="*/3 12-21 * * *", 
    catchup=False,
    start_date=days_ago(1),
    default_args=default_args,
)
def global_dag():
    @task
    def process():
        redis_hook = RedisHook(redis_conn_id="redis")
        redis_client = redis_hook.get_conn()
        current_time = datetime.now().strftime("%H:%M:%S")
        print(f"Checking Redis at {current_time}")
        
        trigger_lists = []
        
        try:
            olimp = redis_client.get("olimp") or b"1"
            pinn = redis_client.get("pinn") or b"1"
            pari = redis_client.get("pari") or b"1"
            
            olimp_val = int(olimp)
            pinn_val = int(pinn)
            pari_val = int(pari)
            
            if olimp_val * pinn_val * pari_val == 0:
                print('Triggering restarts')
                
               
                trigger_lists.append({
                    "trigger_dag_id": "restart_coordinator",
                    "task_id": "trigger_restart_coordinator"
                })
                
                if olimp_val == 0:
                    trigger_lists.append({
                        "trigger_dag_id": "restart_olimp",
                        "task_id": "trigger_restart_olimp"
                    })
                    redis_client.setex("olimp", 600, 1)
                    
                if pari_val == 0:
                    trigger_lists.append({
                        "trigger_dag_id": "restart_pari",
                        "task_id": "trigger_restart_pari"
                    })
                    redis_client.setex("pari", 600, 1)
                    
                if pinn_val == 0:
                    trigger_lists.append({
                        "trigger_dag_id": "restart_pinn",
                        "task_id": "trigger_restart_pinn"
                    })
                    redis_client.setex("pinn", 600, 1)
            else:
                print('All systems operational')
                
        except Exception as e:
            print(f"Redis error: {e}")
            # Consider failing the task here if required
            # raise AirflowException(f"Redis error: {e}")
        
        return trigger_lists

    # Get list of DAGs to trigger from the process task
    trigger_configs = process()

    # Create dynamically mapped TriggerDagRunOperator tasks
    TriggerDagRunOperator.partial(
        task_id="trigger_dag_run",
        wait_for_completion=False
    ).expand_kwargs(trigger_configs)

global_dag_instance = global_dag()
```
</details>

---
<br>

<a id="Catboost"></a>
### ğŸ§  Sports Odds Modeling (*CatBoost, ClickHouse*)
This project uses historical match odds and results to train a machine learning model that predicts whether a placed bet was successful. The data is pulled from a ClickHouse database, cleaned and enriched with custom features such as odds ratios and team roles (favorite vs underdog). A CatBoost classifier is then trained to identify patterns and estimate bet outcomes, helping to evaluate profitability strategies based on historical data.

<details>
<summary>Catboost code</summary>
  
```python
import clickhouse_connect
import pandas as pd
import matplotlib.pyplot as plt
import shap
import numpy as np
from scipy.stats import skellam
from scipy.optimize import brentq
import math
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, accuracy_score


# ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº ClickHouse
client = clickhouse_connect.get_client(
**************************************
)

# SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
query = "SELECT * FROM prematch WHERE timestamp BETWEEN NOW() - INTERVAL '130 days' AND NOW()"


# Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² DataFrame
prematch = client.query_df(query)

query = "SELECT * FROM results WHERE date BETWEEN NOW() - INTERVAL '130 days' AND NOW()"

# Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ² DataFrame
results = client.query_df(query)

prematch = prematch[(~prematch['1'].isna()) & (~prematch['2'].isna())]

prematch['date'] = prematch['timestamp'].dt.date
prematch.drop_duplicates(subset = ['event','date'],inplace = True)
prematch.head()

results['date'] = pd.to_datetime(results['date']).dt.date
results.drop_duplicates(subset = ['event','date'],inplace = True)

df = pd.merge(prematch,results, how = 'inner', on = ['event','date'])

df['has_draw'] = df['X'].notna()  # True â€” 3 Ğ¸ÑÑ…Ğ¾Ğ´Ğ°, False â€” 2 Ğ¸ÑÑ…Ğ¾Ğ´Ğ°

df = df[df['has_draw']]
df = df.reset_index(drop = True)
df.drop(columns = ['has_draw'], inplace = True)
df = df[(df['F'].notna()) & (df['F1'].notna())]
cols_to_check = ["X", "1", "2",'category_x']
df = df[(df[cols_to_check] != 0).all(axis=1) & df[cols_to_check].notna().all(axis=1)]

# Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ
subcategory_parts = df['subcategory_x'].str.split('.')
max_parts = subcategory_parts.map(len).max()

# Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°Ñ„Ñ€ĞµĞ¹Ğ¼ Ğ¸Ğ· Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹
subcategory_df = subcategory_parts.apply(
    lambda x: pd.Series(x + [None] * (max_parts - len(x)))
)
subcategory_df.columns = [f'subcat_{i}' for i in range(max_parts)]

# ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ df
df = df.join(subcategory_df)
df.drop(
    columns=[
        'category_y', 'subcategory_y', 'f1', 'f2', 'total', 'subcategory_x',
        'score1', 'score2','subcat_3', 'subcat_4', 'subcat_5', 'subcat_6', 'timestamp', 'date'
    ],
    inplace=True,
    errors='ignore'
)

df['odds_A'] = df['1']
df['odds_B'] = df['2']

# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ-Ğ°Ğ½Ğ´ĞµÑ€Ğ´Ğ¾Ğ³Ğ°
df['underdog_team'] = df.apply(lambda row: row['team1'] if row['odds_A'] > row['odds_B'] else row['team2'], axis=1)

# Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ°, ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
df['favorite_team'] = df.apply(lambda row: row['team1'] if row['odds_A'] < row['odds_B'] else row['team2'], axis=1)

# Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸
df['odds_underdog'] = df[['odds_A', 'odds_B']].max(axis=1)  # Ñ‡ĞµĞ¼ Ğ²Ñ‹ÑˆĞµ ĞºÑÑ„ â€” Ñ‚ĞµĞ¼ Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑˆĞ°Ğ½Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ Ğ°Ğ½Ğ´ĞµÑ€Ğ´Ğ¾Ğ³
df['odds_favorite'] = df[['odds_A', 'odds_B']].min(axis=1)
df['odds_ratio'] = df['odds_favorite'] / df['odds_underdog']
df['odds_diff'] = df['odds_favorite'] - df['odds_underdog']

df['odds_underdog'] = 1 / df['odds_underdog']
df['odds_favorite'] = 1 / df['odds_favorite']

df['target'] = (
    ((df['stavka'] == '1') & (df['1'].astype(float) > df['2'].astype(float))) | 
    ((df['stavka'] == '2') & (df['2'].astype(float) > df['1'].astype(float)))
)
df['1'] = 1 / df['1'].astype(float)
df['X'] = 1 / df['X'].astype(float)
df['2'] = 1 / df['2'].astype(float)

df = df[~df['underdog_team'].str.contains('Ñ‚Ğ°Ğ¹Ğ¼|Ğ¼Ğ°Ñ‚Ñ‡|ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ğµ|Ñ…Ğ¾Ğ·ÑĞµĞ²Ğ°|Ğ³Ğ¾ÑÑ‚Ğ¸|Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹', case=False, na=False)]
df = df[~df['favorite_team'].str.contains('Ñ‚Ğ°Ğ¹Ğ¼|Ğ¼Ğ°Ñ‚Ñ‡|ÑƒĞ³Ğ»Ğ¾Ğ²Ñ‹Ğµ|Ñ…Ğ¾Ğ·ÑĞµĞ²Ğ°|Ğ³Ğ¾ÑÑ‚Ğ¸|Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹', case=False, na=False)]

cols_to_check = ["X", "1", "2"]
df = df[(df[cols_to_check] != 0).all(axis=1) & df[cols_to_check].notna().all(axis=1)]

sports_with_home_matches = [
    'Ğ¤ÑƒÑ‚Ğ±Ğ¾Ğ»',
    'Ğ¥Ğ¾ĞºĞºĞµĞ¹',
    'Ğ¥Ğ¾ĞºĞºĞµĞ¹ Ñ Ğ¼ÑÑ‡Ğ¾Ğ¼',
    'Ğ¤ÑƒÑ‚Ğ·Ğ°Ğ»',
    'Ğ“Ğ°Ğ½Ğ´Ğ±Ğ¾Ğ»',
    'Ğ¤Ğ»Ğ¾Ñ€Ğ±Ğ¾Ğ»',
    'Ğ’Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğ¾',
    'Ğ¥Ğ¾ĞºĞºĞµĞ¹ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ²Ğµ',
    'Ğ ĞµĞ³Ğ±Ğ¸',
    'Ğ“ÑĞ»ÑŒÑĞºĞ¸Ğ¹ ÑĞ¿Ğ¾Ñ€Ñ‚',
    'Ğ¥Ğ¾ĞºĞºĞµĞ¹ Ğ½Ğ° Ñ€Ğ¾Ğ»Ğ¸ĞºĞ°Ñ…',
    'ĞŸĞ»ÑĞ¶Ğ½Ñ‹Ğ¹ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»'
]
def home_underdog(row):
    if row['category_x'] not in sports_with_home_matches:
        return 'not applicable'
    if row['team1'] == row['underdog_team']:
        return 'home'
    else:
        return 'away'
df['home_underdog'] = df.apply(home_underdog, axis = 1)


def expected_advantage(f1, f2, f):
    """
    Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ° (E[G_fav - G_und]) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² F1, F2 Ğ¸ Ñ„Ğ¾Ñ€Ñ‹ F.
    
    ĞÑ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹:
        f1 (float): ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ½Ğ° Ñ„Ğ¾Ñ€Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ 1.
        f2 (float): ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ½Ğ° Ñ„Ğ¾Ñ€Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ 2.
        f (float): Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ñ‹ (Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ â€” ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° 1 Ğ°Ğ½Ğ´ĞµÑ€Ğ´Ğ¾Ğ³, Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ â€” ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° 2 Ğ°Ğ½Ğ´ĞµÑ€Ğ´Ğ¾Ğ³).
    
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚:
        float: ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ° (Ğ½ĞµĞ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ).
    """
    # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸
    p1 = 1 / f1
    p2 = 1 / f2
    
    # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸
    total = p1 + p2
    p1_normalized = p1 / total
    p2_normalized = p2 / total
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ° Ğ¿Ğ¾ Ğ·Ğ½Ğ°ĞºÑƒ Ñ„Ğ¾Ñ€Ñ‹
    if f > 0:
        # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° 2 â€” Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° 1 â€” Ğ°Ğ½Ğ´ĞµÑ€Ğ´Ğ¾Ğ³
        is_team2_favorite = True
        p_fav = p2_normalized  # P(G2 - G1 > f)
        k = math.ceil(f)  # ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ G2 - G1
    elif f < 0:
        # ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° 1 â€” Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° 2 â€” Ğ°Ğ½Ğ´ĞµÑ€Ğ´Ğ¾Ğ³
        is_team2_favorite = False
        p_fav = p1_normalized  # P(G1 - G2 > |f|)
        k = math.ceil(abs(f))  # ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ G1 - G2
    else:  # f == 0
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ„Ğ°Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ° Ğ¿Ğ¾ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼
        is_team2_favorite = f2 < f1
        p_fav = p2_normalized if is_team2_favorite else p1_normalized
        k = 1  # ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ±ĞµĞ´Ñ‹ (G_fav - G_und > 0)
    
    # Ğ”Ğ»Ñ Ñ€Ğ°Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ñ‹ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ 0
    if f == 0 and abs(f1 - f2) < 0.01:
        return 0.0
    
    # ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ¾Ğ»Ğ¾Ğ²
    total_goals = 3.0  # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ
    
    # Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Î´ = Î»_fav - Î»_und
    def objective(delta):
        lambda_und = (total_goals - delta) / 2
        lambda_fav = (total_goals + delta) / 2
        if lambda_und < 0 or lambda_fav < 0:
            return np.inf
        # Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ P(G_fav - G_und >= k)
        prob = 1 - skellam.cdf(k - 1, lambda_fav, lambda_und)
        return prob - p_fav
    
    # Ğ§Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ
    try:
        delta = brentq(objective, 0, total_goals - 0.001)
    except ValueError:
        # ĞŸÑ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ: Î´ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ |f| Ğ¸ p_fav
        delta_approx = abs(f) * (p_fav / (1 - p_fav)) if p_fav < 0.99 else abs(f) * 2
        delta = max(delta_approx, 0)
    
    return delta


# Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾
df['expected_advantage'] = df.apply(lambda row: expected_advantage(row['F1'], row['F2'], row['F']), axis=1)

df.drop(columns = ['1','2','event','F1','F2','F','Tb','Tm','T'],inplace = True)
df.drop(columns = ['stavka','odds_A','odds_B','team1','team2'],inplace = True)

df.rename(columns={'category_x': 'category'}, inplace=True)

df.reset_index(drop = True)



# Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ
target = 'target'

# ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¸Ñ‡Ğ¸
cat_features = [
    'category', 'subcat_0', 'subcat_1',
       'subcat_2', 'home_underdog' # <--- Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸
]

# Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¸Ñ‡Ğ¸ Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸
features = [col for col in df.columns if col not in [target,'underdog_team','favorite_team']]

# Ğ”ĞµĞ»Ğ¸Ğ¼ Ğ½Ğ° train/test
X_train, X_test, y_train, y_test = train_test_split(
    df[features], df[target], test_size=0.2, random_state=42
)

X_train = X_train.fillna(-999)
X_test = X_test.fillna(-999)

# Ğ¤Ğ¸ĞºÑĞ¸Ğ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº cat_features (Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ ĞµÑÑ‚ÑŒ Ğ² X_train)
cat_features_in_data = [col for col in cat_features if col in X_train.columns]

# ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
model = CatBoostClassifier(
    iterations=3000,  
    learning_rate=0.02,  
    depth=6,  
    l2_leaf_reg=8,  
    loss_function='Logloss',
    eval_metric='Logloss',
    verbose=200,
    random_seed=42,
    early_stopping_rounds=100,
    bagging_temperature=2,  
    random_strength=3,  
    bootstrap_type='MVS',  
    grow_policy='Lossguide', 
    max_leaves=100,  
    min_data_in_leaf=70,  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ»Ğ¸ÑÑ‚ÑŒÑÑ…
    subsample=0.85,  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
)

model.fit(X_train, y_train, cat_features=cat_features_in_data, eval_set=(X_test, y_test))
```
</details>

<br>

<a id="experience-section"></a>

## ğŸ’¼ Work Experience & Achievements

- ğŸ¢ **11 years of experience in the petrochemical industry**  
  - **Supply chain management, business analytics, and data analytics**.  
- ğŸ“ **MBA Graduate from RANEPA**  
  - Accredited by **AMBA** and **AACSB**.  
- ğŸŒ **Internship in Germany (Deutsche Management Akademie Niedersachsen, Hansa-Flex)**  
- ğŸ“Š **IELTS 8.0 (C1 English)**  
- ğŸ“œ **IBM Data Engineer Professional Certificate**  
---
<br>
<a id="skills-section"></a>

## ğŸ› ï¸ Skills

### ğŸ‘¨â€ğŸ’» Programming & Scripting
- **Python (Pandas, NumPy, Matplotlib, Seaborn, Selenium, BeautifulSoup, regex)**  
- **SQL (PostgreSQL, ClickHouse, BigQuery)**
- **Bash**

### ğŸ“Š Data Visualization Tools
- **Tableau**
- **Power BI**  
- **Metabase**  
- **Grafana**  

### â˜ï¸ Cloud & Other Technologies
- **AWS, GCP**  
- **Docker**  
- **Airflow**  

---
<br>
<a id="contacts-section"></a>

## ğŸ“¬ Contact Me

<p align="left">
  <a href="https://t.me/poi8lkj" target="_blank">
    <img src="https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white" alt="Telegram">
  </a>
  <a href="https://www.linkedin.com/in/georgii-romanov-3a62a526a/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn">
  </a>
</p>
